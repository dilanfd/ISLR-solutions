---
title: "Chapter 2 Solutions"
author: "Dilan Fernando"
date: "`r format(Sys.Date())`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

----------

 **Conceptual Questions**
 
## 2.4.1
Let $n$ = sample size and $p$ = number of predictors.

(a) $n$ is extremely large and $p$ is small

	*Answer*: Flexible is better because we have a lot of training
	data and an inflexible model will have too much bias that can't
	be overcome with the large amount of data points we have.

(b) $p$ is extremely large and $n$ is small.

	*Answer*: An inflexible model will do better because we don't have
	a lot of data (n) to work with. The flexible model will introduce
	large amount of variance to the model and will overfit the data.

(c) The predictor response relationship is highly non-linear.

	*Answer*: The flexible method will do better because it can
	potentially capture all of the subtleties and non linearities
	in the data. An inflexible model simply won't have the necessary
	number of variable to catch the non linearities.
	
(d)	The variance of the error terms $\sigma^2 =
                                     \mathrm{Var}(\epsilon)$ is
                                     extremely high.
	
	*Answer:* Inflexible methods are preferred. If we used flexible
	methods we will simply model the noise in the data and overfit
	our model.
	
----------

## 2.4.2

(a) Regression. Interested in inference. $n = 500$ and \(p = 3\)
	
(b) Classification. Prediction. n = 20 and p = 13.

(c) Regression. Prediction. n = 52 * 3 and p = 3.


----------


## 2.4.3 (Bias vs Variance decomposition)

(a)

```{r echo=FALSE, out.width='50%'}
knitr::include_graphics('./figures/bias-variance.JPG')
```

(b) 
  * 
  * `variance`

  As model flexibility increases it starts go after
  (using Hastie vernacular) noise in the data. This results in an
  overfitting model. So if we estimate $\hat f$ using a different
  training set we will end up with a different $\hat f$ resulting
  in a higher `variance`.
	
  * squared `bias`
  
  Bias is introduced when we estimate a potentially complicated real-
  life problem using a much simpler model. So simple models will have
  higher bias. Naturally, as model flexibility increase this bias will
  decrease.
  
  * `training error`
  
  The more flexible the model the less the training error should be.
  It gradually decreases as it gradually does better and better at
  even predicting noise in the data. 
  
  * `test error`
  
  Test error will decrease as model complexity increase only upto
  a certain point. After this point, certain models which are
  overfitting the data will not be able to give good predictions
  to unseen data resulting in a larger test error. 

  * `irreducible error`
	
  Irreducible error by definition is unaffected by model flexibility
  (or any other external force). It remains constant no matter what.
  
----------

## 2.4.4

(a) **Classification examples**

	* flagging a financial transaction as fraudulent or not.
	This is a classification prediction problem.
	
	* flagging a loan application as being at risk for default or not.
	The end goal is prediction.
	
	* Trying to understand which demographic/personal data contribute
	to determining possible loan defaulters. This is a classification
	problem with the end goal of inference.
	
(b)	** Regression **
	
	* Trying to predict next years sales using this years budget on
	advertising. This is a prediction.
	
	* Trying to understand which media (radio/tv/newspaper) contribute
	to sales. This is a regression problem whose end goal is
	inference.
	
	* Trying to predict the price of a stock using past data. The goal
	is prediction using regression. (of course this is a naive
	example).

(c)	**Cluster Analysis**

	* Market segmentation of identifying types of customers.
   
	* Determining terrorist threat levels (low, moderate, high)
	using NLP.
	
	* Segmenting a population as being at high risk, low risk
	and moderate for heart desease on demographic and personal
	data.

----------

## 2.4.5

**More flexible vs less flexible**
	
A very flexible approach may be useful if we have a large number
of training data (in the thousands) with only a handful of predictor
variables. The reason being, with the large number of training data
we will be able to capture all of the subtleties of the data set
without overfitting. This of course works under the standing
assumptions that the `real relationship is not linear`. On the
other hand if we had only a small number of data points and a
relatively large number of features, then a less flexible model
will be the better choice. This is because a more flexible model
will go after the noise in the data and will overfit. 

## 2.4.6

**Parametric vs Non parametric**

Parametric methods (regression/logistic models etc) are interpretable
whereas non parametric methods (k nearest neighbors/SVM etc) are
difficult to interpret. It all depends on the type of problem one
is trying to solve. For example if we are trying to predict stock
prices then a non interpretable model is fine as long as it gives
us correct results. On the other hand if our goal is inference, then
we may need to know how features are related (if at all). In such
cases, interpretability of our model plays a role.


## 2.4.7


```{r}
eucl_dist <- function(x, y, z){
    ## compute l2 distance.
    return(sqrt(sum(x^2, y^2, z^2)))
}
```
(a) 

- Distance between $(0, 3, 0)$ and $(0, 0, 0)$ is `r round(eucl_dist(0,3,0), 2)`

- Distance between $(2, 0, 0)$ and $(0, 0, 0)$ is `r round(eucl_dist(2,0,0), 2)`

- Distance between $(0, 1, 3)$ and $(0, 0, 0)$ is `r round(eucl_dist(0,1,3), 2)`
  
- Distance between $(0, 1, 2)$ and $(0, 0, 0)$ is `r round(eucl_dist(0,1,2), 2)`

- Distance between $(-1, 0, 1)$ and $(0, 0, 0)$ is `r round(eucl_dist(-1, 0,1), 2)`
  
- Distance between $(1, 1, 1)$ and $(0, 0, 0)$ is `r round(eucl_dist(1,1, 1), 2)`

(b)

$K = 1$ means that we want the nearest neighbor. Since the
smallest euclidean distance is the one that corresponds to observation
6 whose prediction is `Green`. So we assign the prediction `Green ` to
$(0, 0, 0)$.

(c) 

 **Intuitively**
 
$K=3$ means we look at the its three nearest neighbors and assign the
conclusion that arises the most out of the three. The three neighbors
are,

\begin{align}
x_2 & = (2, 0, 0): & \quad \text{Red} \\
x_5 & = (-1, 0, 1): & \quad \text{Green} \\
x_6 & = (1, 1, 1): & \quad \text{Red}
\end{align}

Clearly the response that arises the most is `Red`and so we assign
`Red` as the assignment for the point $(0, 0, 0)$.

 **Mathematically**
 
$K = 3$ means that we choose those points that are closest to 
\(x_0 = (0, 0, 0)\). We then have \(\{x_2, x_5, x_6\} = \mathcal{N}_0\).
Since `red` and `green` are the only possible responses, we check
them. More precisely,

\begin{align}
\mathrm{Pr}(Y = Red | X = x_0) & = \frac{1}{K}\sum_{i \in \mathcal{N}_0}
I(y_i = Red) & = \frac{1}{3}\left(1 + 0 + 1\right) & = \frac{2}{3}\\
\mathrm{Pr}(Y = Green | X = x_0) & = \frac{1}{K}\sum_{i \in \mathcal{N}_0}
I(y_i = Green) & = \frac{1}{3}\left(0 + 1 + 0\right) & = \frac{1}{3}\\
\end{align}

Since the larger of the two probabilities is \(2/3\) we assign `Red`
as response for `(0, 0, 0)`. QED.

(c) If the Bayes decision boundary were highly non linear we would
expect the value of `K` to be small. We would like to capture all
of non linearity and so we do not want to go too far out of our
immediate neighbors. The effect is that we will want to stay pretty
close to our neighborhood. So choose a small `K` value.

----------

 **Applied Questions**

## 2.4.8
```{r}
library("ISLR")
attach(College) # foo instead of College$foo
```



(a) - (b) already done in the `ISLR` package. So nothing to do here.


(c) 
	(i)
	```{r}
	summary(College)
    ```
	
	(ii)
	```{r}
        # scatter plots of the first 10 columns
        pairs(College[, 1:10]) 
	```

	(iii)
	```{r}
        ## A box plot of Out of state tuition on y axis and whether private or not on x axis.
        boxplot(Outstate ~ Private,
             main = "Out of state tuition",
             ylab = "Out of state tuition",
             col = "orange",
             xlab = "Private College")
	 ```

	(iv) 
	
	Create a new variable `Elite` by _binning_ the `Top10perc`
	variable according as follows:
	
	Elite = at least 50% of the students are in the top 10% of
	highschool and not Elite otherwise.
	

```{r}	

Elite <- rep("No", nrow(College))
Elite[Top10perc > 50] <- "Yes"
Elite <- as.factor(Elite)
college_new <- data.frame(College, Elite)

## Attach the college_new
attach(college_new)

## Count the number of elite universities.
summary(Elite)
```

	There are 78 Elite Universities according to our criteria.

```{r}
plot(Outstate ~ Elite,
             main = "Out of State tuition for Elite Universities",
             ylab = "Tuition",
             col = "orange",
             xlab = "Elite")
```


(v) 
	```{r}
        par(mfrow = c(2, 2)) # Create a 2 x 2 grid for plots to go in.
        hist(Expend, breaks = "FD", col = 2)
        hist(S.F.Ratio, breaks = "FD", col = 4)
        hist(Grad.Rate, breaks = "FD", col = 6)
        hist(PhD, breaks = "FD", col = 8)
	```


	(vi) 
	Note from the previous histogram that the Grad.Rate and the % of
	faculty with Phd's exceed  100%. This is peculiar. Let us explore.
	
	```{r}
summary(PhD)
odd.PhD <- which(college_new$PhD == 103)
odd.PhD
rownames(college_new)[odd.PhD]
	```

Turns out we have 103% of the faculty at TAMU Galveston have PhD's.
The likely explanation is that there is a data anomaly or a mistake
in data entry and it needs to be cleaned before doing any data
analysis on this data.

----------

## 2.4.9
```{r}
library(ISLR)
attach(Auto)
```
(a) 

```{r}
str(Auto)
```

The variable "name" and "horsepower" is qualitative. All of the other 
variables are quantitative.

(b)
```{r}
## Get range of every variable.
summary(Auto)
```

(c)
```{r}
## Mean.
sapply(Auto[, -c(4, 9)], mean)

## Standard Deviation.
sapply(Auto[, -c(4, 9)], sd)
```

----------
 **Remark** Notice the use of the `sapply` function. It is an
 extremely useful function that can be used instead of writing
 a `for loop` for this type of manipulation. See `?sapply` on
 R documentation.
----------

(d)

```{r}
## remove rows 10 thru 85
row_rem_auto <- Auto[-seq(10:85), ]
row_rem_auto

## Find the new ranges.
summary(row_rem_auto)

## Find the new mean and sd.
sapply(row_rem_auto[, -c(4, 9)], mean)
sapply(row_rem_auto[, -c(4, 9)], sd)
```

(e) 
	**EDA of the variables.**

  * Create pairwise plots of the variables using `pairs`.
  
```{r}
pairs(Auto)
```

 **Observations**
 
  * `horsepower` and `displacement` seem to have a linear relationship
    (correlated)	
  * `horsepower`and `weight` seem to be strongly correlated as well.
  
  * `weight` and `displacement` are also strongly correlated.
  
  * `displacement`, `horsepower`, and `weight` variables seem to be
   highly correlated with the `mpg` (response)
   variable. `acceleration` also seem to be correlated to `mpg` but to
   a lesser extent than others.
   
(f) In light of the above observations, we may be able to find a 
strong linear regression relationship such as 


```
mpg ~ displacement + horsepower + weight
```
----------


## 2.4.10
```{r}
library(MASS)
str(Boston)
Boston$chas <- as.factor(Boston$chas)
```

(a) 
This data set contains housing values of suburbs of Boston. There are
506 (rows) observations and 14 (columns) variables. `Columns` represent
the following variables. `Rows` represent *instances* of these variables.

	- ‘crim’ per capita crime rate by town.

	- ‘zn’ proportion of residential land zoned for lots over 25,000
	sq.ft.

	- ‘indus’ proportion of non-retail business acres per town.
   
	- ‘chas’ Charles River dummy variable (= 1 if tract bounds river; 0
     otherwise).

	- ‘nox’ nitrogen oxides concentration (parts per 10 million).
	rm average number of rooms per dwelling.

	- ‘age’ proportion of owner-occupied units built prior to 1940.

	- ‘dis’ weighted mean of distances to five Boston employment centres.

	- ‘rad’ index of accessibility to radial highways.

	- ‘tax’ full-value property-tax rate per \$10,000.

	- ‘ptratio’ pupil-teacher ratio by town.

	- ‘black’ 1000(Bk - 0.63)^2 where Bk is the proportion of blacks
	by town.

	- ‘lstat’ lower status of the population (percent).

	- ‘medv’ median value of owner-occupied homes in \$1000s.

(b) 
```{r}
pairs(Boston)


## Look at all pairs first. Then decide on the ones to plot.
par(mfrow = c(2, 2))
plot(Boston$medv, Boston$crim)
plot(Boston$dis, Boston$crim)
plot(Boston$age, Boston$crim)
plot(Boston$lstat, Boston$crim)

## Reset the plot window to 1
par(mfrow = c(1,1))
```
(c)

The predictors `medv`, `dis`, `age` and `lstat` seem to be predictors
associated with per capita crime rate.

 - `medv` negatively correlated.
 - `dis` negatively correlated.
 - `age` positively correlated.
 - `lstat` positively correlated.
 
 
(d)
 * Crime rates by suburbs. 
```{r}
## Find the range.
range(Boston$crim)
## 
boxplot(Boston$crim)
```


The range for crime rates is `r range(Boston$crim)`. Looking at the
box plot it is clear that some suburbs have high crime rates.


 * Tax rates by suburbs
```{r}
range(Boston$tax)
boxplot(Boston$tax)
```
There are no particularly high tax rate suburbs considering
the fact that the `boxplot` is fairly evenly spread between
the `max` and the `min`.

 * Pupil-teacher ratios
```{r}
range(Boston$ptratio)
boxplot(Boston$ptratio)
```

Once gain there does not seem to be particularly high `Pupil-teacher`
ratios according to the boxplot.

(e) How many of the rivers bound the Charles river?
```{r}
## subset required rows first. then count rows.
nrow(Boston[Boston$chas == 1, ])
```

(f) What is the median Pupil-teacher ratio among the towns
in this dataset?

```{r}
median(Boston$ptratio)
```

(g) 
Which suburb of Boston has lowest median value of owner occupied
homes?. 
```{r}
which.min(Boston$medv)
```

It looks as if the `r which.min(Boston$medv)` th suburbs of Boston
have the lowest such median value. What if there are two minimum
values?. We can answer it as follows: **Subset all such rows**

```{r}
attach(Boston)
Boston[medv == min(medv),]
```

It turns out that the 399th as well as the 406th suburbs both
have lowest median value of owner occupied homes. Also note that
the `crim` variable has range `r range(crim)`. Both the suburbs
(and in particular 406th) have high crime rates. 

The pupil teacher ratio is also high in these two particular 
suburbs compared to the overall range `r range(ptratio)`. 

Surprisingly tax rate also seem to be high in these areas. One
would expect such areas to have lower tax rates.

(f)

Average number of rooms per dwelling is given by the `rm` variable. 

`r nrow(Boston[Boston$rm > 7, ])` number of suburbs average more than
7 rooms per dwelling and `r nrow(Boston[Boston$rm > 8, ])` number of
dwellings average more than 8.

 **The suburbs that average more than 8 rooms per dwelling**
 
```{r}
large_room_Boston <- Boston[Boston$rm > 8,]
large_room_Boston
range(large_room_Boston$crim)
range(Boston$crim)
median(large_room_Boston$crim)
mean(Boston$crim)
mean(large_room_Boston$crim)
```

There seem to be relatively low crime rates amount the suburbs that
average more than 8 rooms per dwelling as expected. The more rooms
is usually equivalent to affluent neighborhoods.

----------

 **END OF CHAPTER** 
